{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí voy a definir mediante un notebook, paso a paso, el train_LSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install scikit-learn\n",
    "# %pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 21:20:05.816936: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-28 21:20:05.873989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-28 21:20:06.878074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se detectó ninguna GPU.\n",
      "2.16.1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 21:20:09.385700: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow is an open-source ML framework developed by Google\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5000)]\n",
    "        )\n",
    "        print(\"We limit the memory of the GPU to 5000MB\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error al configurar la GPU:\", e)\n",
    "else:\n",
    "    print(\"No se detectó ninguna GPU.\")\n",
    "\n",
    "\n",
    "# Python library for scientific computing (arrays)\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix used to evaluate the performance of the model\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# A module that allows us to interact with the OS\n",
    "import os\n",
    "\n",
    "print(tf.__version__) # This shows the tensorflow version\n",
    "print(tf.config.list_physical_devices()) \n",
    "\n",
    "# Keras is a Python high-level NNs API (application programming interface) integrated into TensorFlow\n",
    "# Import regularization techniques (L1, L2...) \n",
    "from tensorflow.keras                       import regularizers\n",
    "\n",
    "# Import the Sequential and Model classes\n",
    "    # a) Sequential allows for linear stacking of layers\n",
    "    # b) Model provides more flexibility for defining complex NN architectures\n",
    "from tensorflow.keras.models                import Sequential, Model\n",
    "\n",
    "# Import all layer classes from tensorflow.keras.layers (Dense, Conv2D, MaxPooling2D...)\n",
    "from tensorflow.keras.layers                import *\n",
    "\n",
    "# Import ModelCheckpoint and ReduceLROnPlateau classes\n",
    "    # These classes are usually called during training\n",
    "    # a) ModelCheckpoint is used to save model weights\n",
    "    # b) ReduceLROnPlateau is used to reduce the learning rate \n",
    "    # If the monitored metric did not improve after some epochs, the learning rate will be reduced\n",
    "    # Let us assume the \"factor\" parameter is 0.1, then the learning rate will be reduced by a factor of 10\n",
    "    # We can define whether we want it to get activated when the metric has stopped improving or when it has stopped decreasing\n",
    "from tensorflow.keras.callbacks             import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Import the SGD (Stochastic Gradient Descent) to minimize the loss function during the training of a NN\n",
    "from tensorflow.keras.optimizers            import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se definen algunas funciones que se utilizarán más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be useful after your model has been trained, during the evaluation phase, to visualize how well your model performs per class. It builds a confusion matrix\n",
    "def getConfusionMatrix(model, dataset):\n",
    "   \n",
    "    \"\"\"\n",
    "    Generate and display a confusion matrix for a trained model on a given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        model (tf.keras.Model): Trained TensorFlow/Keras model.\n",
    "        dataset (tf.data.Dataset): Dataset to evaluate (should yield image batches and one-hot encoded labels).\n",
    "\n",
    "    Returns:\n",
    "        sklearn ConfusionMatrixDisplay object (also displays the matrix).\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = []  # List to store predicted class indices\n",
    "    y_true = []  # List to store true one-hot labels\n",
    "\n",
    "    # Iterate over the dataset (batch-wise)\n",
    "    for image_batch, label_batch in dataset:   # use dataset.unbatch() with repeat\n",
    "        \n",
    "        y_true.append(label_batch) # Store true labels\n",
    "        preds = model.predict(image_batch) # Get model predictions\n",
    "        y_pred.append(np.argmax(preds, axis = - 1)) # Convert predictions to class indices\n",
    "\n",
    "    # Convert the true and predicted labels into tensors\n",
    "    correct_labels = tf.concat([item for item in y_true], axis = 0)\n",
    "    predicted_labels = tf.concat([item for item in y_pred], axis = 0)\n",
    "\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    final_correct_labels = []\n",
    "    for lb in correct_labels:\n",
    "        itlist=list(lb.numpy())\n",
    "        final_correct_labels.append(itlist.index(1.0))\n",
    "\n",
    "    return ConfusionMatrixDisplay.from_predictions(final_correct_labels, predicted_labels, display_labels=class_names, cmap=plt.cm.Blues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, logModelTrainFinished(...), is meant to be executed after training the model. It saves your final model, logs a confusion matrix \n",
    "# tracks misclassified examples, stores all this into Neptune.ai for experiment tracking\n",
    "# It’s a post-training logging utility, essential for experiment reproducibility and understanding model behavior.\n",
    "\n",
    "def logModelTrainFinished(run, model, validation_ds, class_names): \n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        run: Neptune run object (used to log files and metrics)\n",
    "        model: your trained Keras model\n",
    "        validation_ds: dataset used to evaluate performance (ideally validation or test)\n",
    "        class_names: list of string names of classes \n",
    "    \"\"\"\n",
    "    os.makedirs(\"model_trainer\", exist_ok=True) # To create a folder (model_trainer) to store temporary files (models, confusion matrix, misclassified images)\n",
    "\n",
    "    model.save(\"model_trainer/current_run_model.keras\") # Save the latest model \n",
    "    run[\"model_last.h5\"].upload(\"model_trainer/current_run_model.keras\") # Upload it to Neptune\n",
    "\n",
    "    getConfusionMatrix(model, validation_ds).figure_.savefig(\"model_trainer/confusion_matrix.png\") # Calls the previous function getConfusionMatrix(...) to generate the confusion matrix figure. Saves the figure as an image\n",
    "    run[\"eval/conf_matrix\"].upload(\"model_trainer/confusion_matrix.png\") # Uploads it to Neptune \n",
    "\n",
    "    # If folder missclasified exists delete it (por si hay entrenamientos previos, borras lo anterior)\n",
    "    if os.path.exists(\"model_trainer/missclasified\"):\n",
    "        os.system(\"rm -rf model_trainer/missclasified\")\n",
    "    # Create folder missclasified\n",
    "    os.makedirs(\"model_trainer/missclasified\", exist_ok=True)\n",
    "\n",
    "    cubatch = 0 # Count batchs to generate unique names (para que salga info del batch en el nombre de la imagen)\n",
    "    # Generate missclasified images\n",
    "    for image_batch, label_batch in validation_ds:   # train_ds or validation_ds\n",
    "    \n",
    "        # Compute predictions\n",
    "        preds = model.predict(image_batch)\n",
    "\n",
    "        # Dump and upload missclasified images\n",
    "        for i in range(len(preds)):\n",
    "            if np.argmax(preds[i]) != np.argmax(label_batch[i]):\n",
    "                #save image\n",
    "                imgName = \"batch_\" + str(cubatch) + \"_img_\" + str(i) +  \"_true_\" + class_names[np.argmax(label_batch[i])] + \"_pred_\" + class_names[np.argmax(preds[i])] + \".png\"\n",
    "                imgPath = \"missclasified/\" + imgName\n",
    "                plt.imsave(\"model_trainer/\" + imgPath, image_batch[i].numpy())\n",
    "                run[\"eval/\"+imgPath].upload(\"model_trainer/\" + imgPath)\n",
    "    \n",
    "        cubatch += 1\n",
    "\n",
    "    try:\n",
    "        accuracy_dataframe = run[\"eval/accuracy\"].fetch_values() #para obtener el accuracy subido a neptune\n",
    "    except:\n",
    "        accuracy_dataframe = []\n",
    "        \n",
    "    # get num of epochs executed\n",
    "    run[\"train/epochs_executed\"] = len(accuracy_dataframe) # Guardar los epochs ejecutados en neptune\n",
    "    run[\"model_params_count\"] = model.count_params() # Guardar el numero de parámetros en neptune\n",
    "\n",
    "\n",
    "    # Guardar el \"mejor\" modelo según accuracy, encontrado con el checkpointer más arriba. Antes solo se guardaba en la carpeta \"model\" del servidor, pero esto desaparece con cada entrenamiento. \n",
    "    # Primero cargamos ese modelo\n",
    "    model = tf.keras.models.load_model(\"model_best.keras\")\n",
    "\n",
    "    # Guardamos el modelo nuevo como un \".keras\" para subirlo después a Neptune\n",
    "    model.save(\"model_trainer/best_run_model.keras\") #guardar el modelo\n",
    "    run[\"best_model\"].upload(\"model_trainer/best_run_model.keras\") #subir el modelo a neptune\n",
    "\n",
    "    first = accuracy_dataframe.iloc[0] \n",
    "    last = accuracy_dataframe.iloc[-1]\n",
    "\n",
    "    # get timestamp column from series\n",
    "    first = first[\"timestamp\"]\n",
    "    last = last[\"timestamp\"]\n",
    "\n",
    "    # get difference in seconds\n",
    "    duration_timedelta = last - first\n",
    "\n",
    "    run[\"train/duration_seconds\"] = duration_timedelta.total_seconds()\n",
    "    run[\"train/duration_text\"] = str(duration_timedelta)\n",
    "\n",
    "    # wait for neptune to upload files\n",
    "    run.wait()\n",
    "    run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya pasamos al código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base variables for the project\n",
    "DATASET_BASE_PATH = './Datasets_LSE'\n",
    "DATASET_PATH = DATASET_BASE_PATH + '/Letters192'\n",
    "# TRAINED_MODELS_PATH = '../TrainedModels'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute hidden_neptune.py to get api key\n",
    "neptune_api_token = open(\"./neptune_api_token_LSE.txt\").read()\n",
    "using_neptune = True\n",
    "\n",
    "# %pip install neptune\n",
    "import neptune\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"] = neptune_api_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debo cargar los datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:21:41.230816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:98:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57967 files belonging to 23 classes.\n",
      "Found 6654 files belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "# This cell loads your image dataset from folders into TensorFlow's format (tf.data.Dataset) so it can be used for training and evaluation. \n",
    "\n",
    "bs = 32 # Antes era 64 pero daba errores del GPU # It's the batch size. How many images are loaded per batch during training.\n",
    "image_side = 192 # All input images are resized to 192×192 pixels (standard input size for the model). CREO QUE NO ES NECESARIO (pq aqui ya lo esoty haciendo con las 192x192)\n",
    "\n",
    "# Load dataset, it must contains Train and Test in categorical folders\n",
    "with tf.device('/cpu:0'):\n",
    "  raw_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_PATH + \"/TrainV\",\n",
    "    label_mode = \"categorical\",\n",
    "    shuffle = True,\n",
    "    image_size = (image_side, image_side),\n",
    "    batch_size = bs)\n",
    "\n",
    "  raw_validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_PATH + \"/Validation\",\n",
    "    label_mode = \"categorical\",\n",
    "    shuffle = True,\n",
    "    image_size = (image_side, image_side),\n",
    "    batch_size = bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'CH', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'W', 'X']\n"
     ]
    }
   ],
   "source": [
    "# Hare we are assigning class labels and normalizing your images so the model can learn effectively.\n",
    "# We define class names\n",
    "class_names = raw_train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "# Normalize from 0-255 to 0-1\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = raw_train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "validation_ds = raw_validation_ds.map(lambda x, y: (normalization_layer(x), y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It creates realistic variations of your training images (on the fly) so the model doesn’t overfit to the training data, learns to be robust to noise, rotations, translations, etc.\n",
    "\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.01),\n",
    "        tf.keras.layers.RandomZoom(0.02),\n",
    "        tf.keras.layers.RandomTranslation(0.08, 0.08, fill_mode='nearest', fill_value=0.5),\n",
    "        tf.keras.layers.RandomBrightness([-0.15,0.1],value_range=(0, 1)),\n",
    "        #tf.keras.layers.RandomCrop(25,25),\n",
    "        #tf.keras.layers.RandomContrast(0.02)\n",
    "    ]\n",
    ")\n",
    "data_augmentation.build((None, image_side, image_side, 3)) # This is important to prevent usage of data augmentation change the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we config model params\n",
    "model_params = {\"dropout1\": 0.3, \"dropout2\": 0.2, \"dense\": 74, \"l2reg\": 0.02} # Los de Pamela de para DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/LSE-Sign-Language2/LSE-Sign-Language/\n"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "project = neptune.init_project(project=\"LSE-Sign-Language2/LSE-Sign-Language\", api_token=neptune_api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEPTUNE_API_TOKEN\"] = neptune_api_token\n",
    "os.environ[\"NEPTUNE_PROJECT\"] = \"LSE-Sign-Language2/LSE-Sign-Language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "<Activation name=conv5_block3_out, built=True>\n"
     ]
    }
   ],
   "source": [
    "# Aqui hago una exploración de los modelos para ver que capas congelar durante el Transfer Learning\n",
    "# DENSENET201\n",
    "#base_model = tf.keras.applications.DenseNet201(\n",
    "#     input_shape=(192, 192, 3),\n",
    "#     include_top=False,\n",
    "#     weights='imagenet' )\n",
    "\n",
    "# MOBILENET\n",
    "#base_model = tf.keras.applications.MobileNet(\n",
    "#     input_shape=(192, 192, 3), \n",
    "#     include_top=False, \n",
    "#     weights='imagenet'\n",
    "#     )\n",
    "\n",
    "# MOBILENETV2\n",
    "#base_model = tf.keras.applications.MobileNetV2(\n",
    "#    input_shape=(192, 192, 3), \n",
    "#    include_top=False, \n",
    "#    weights='imagenet'\n",
    "# )\n",
    "\n",
    "# InceptionV3\n",
    "#base_model = tf.keras.applications.InceptionV3(\n",
    "#    input_shape=(192, 192, 3), \n",
    "#    include_top=False, \n",
    "#    weights='imagenet'\n",
    "#)\n",
    "\n",
    "# ResNet152\n",
    "base_model = tf.keras.applications.ResNet152(\n",
    "    input_shape=(192,192,3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "print(len(base_model.layers))\n",
    "# base_model.summary()\n",
    "\n",
    "\n",
    "#for i, layer in enumerate(base_model.layers):\n",
    "#    output_shape = getattr(layer, 'output_shape', 'N/A')\n",
    "#    kernel_size = getattr(layer, 'kernel_size', '')  # Solo aplica a Conv layers\n",
    "#    kernel_str = f\"{kernel_size[0]}x{kernel_size[1]}\" if kernel_size else \"N/A\"\n",
    "\n",
    "#    print(f\"{i:03d} | Name: {layer.name:<25} | Type: {layer.__class__.__name__:<20} | Kernel: {kernel_str:<5} | Output Shape: {output_shape}\")\n",
    "\n",
    "\n",
    "print(base_model.layers[514])   # 44 fin bloque 4 # 80 fin bloque 8 # 115 fin bloque 12 # 153 congelo todo (bloque 16 y las útlimas capas tb)\n",
    "\n",
    "# Cuando ejecute algo aqui, mirar lo de nvidia-sim para ver si se ha quedado colgado el GPU\n",
    "# ps -o user= -p 203699\n",
    "# kill -9 203699\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ densenet201 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1920</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">18,321,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69120</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,423,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,495</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ densenet201 (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1920\u001b[0m)     │    \u001b[38;5;34m18,321,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69120\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m4,423,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)             │         \u001b[38;5;34m1,495\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,747,223</span> (86.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,747,223\u001b[0m (86.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,518,167</span> (85.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,518,167\u001b[0m (85.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,056</span> (894.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m229,056\u001b[0m (894.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = tf.keras.applications.DenseNet201(input_shape=(image_side,image_side,3), include_top=False, weights='imagenet')\n",
    "#base_model = tf.keras.applications.ResNet152(input_shape=(image_side,image_side,3), include_top=False, weights='imagenet')\n",
    "\n",
    "num_layers_to_freeze = 0\n",
    "for layer in base_model.layers[:num_layers_to_freeze]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "  data_augmentation, # Apply augmentation before training\n",
    "  base_model,  # Feature extractor\n",
    "  Flatten(),   # Flatten feature map to vector\n",
    "  Dense(model_params[\"dense\"], kernel_regularizer=regularizers.l2(model_params[\"l2reg\"]), activation = 'relu'),  # Fully connected layer (with L2 regularization)\n",
    "  Dropout(model_params[\"dropout2\"]), # Drop some neurons to avoid overfitting\n",
    "  Dense(len(class_names), activation = 'softmax') # Output layer: one neuron per class\n",
    "])\n",
    "\n",
    "model.build((None, image_side, image_side, 3))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/LSE-Sign-Language2/LSE-Sign-Language/e/LSES-21\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 22:20:33.934503: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6439 - loss: 3.1318\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69206, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1371s\u001b[0m 1s/step - accuracy: 0.6441 - loss: 3.1310 - val_accuracy: 0.6921 - val_loss: 2.9519\n",
      "Epoch 2/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9791 - loss: 1.8796\n",
      "Epoch 2: val_accuracy improved from 0.69206 to 0.73144, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1290s\u001b[0m 1s/step - accuracy: 0.9791 - loss: 1.8796 - val_accuracy: 0.7314 - val_loss: 2.8885\n",
      "Epoch 3/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9889 - loss: 1.7478\n",
      "Epoch 3: val_accuracy improved from 0.73144 to 0.74767, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1283s\u001b[0m 1s/step - accuracy: 0.9889 - loss: 1.7478 - val_accuracy: 0.7477 - val_loss: 2.7674\n",
      "Epoch 4/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9937 - loss: 1.6427\n",
      "Epoch 4: val_accuracy improved from 0.74767 to 0.76555, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1278s\u001b[0m 1s/step - accuracy: 0.9937 - loss: 1.6427 - val_accuracy: 0.7656 - val_loss: 2.5814\n",
      "Epoch 5/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 1.5525\n",
      "Epoch 5: val_accuracy improved from 0.76555 to 0.77142, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1250s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 1.5525 - val_accuracy: 0.7714 - val_loss: 2.5166\n",
      "Epoch 6/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9965 - loss: 1.4664\n",
      "Epoch 6: val_accuracy improved from 0.77142 to 0.77202, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1253s\u001b[0m 1s/step - accuracy: 0.9965 - loss: 1.4663 - val_accuracy: 0.7720 - val_loss: 2.4705\n",
      "Epoch 7/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 1.3890\n",
      "Epoch 7: val_accuracy improved from 0.77202 to 0.78284, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1237s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 1.3890 - val_accuracy: 0.7828 - val_loss: 2.3362\n",
      "Epoch 8/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 1.3144\n",
      "Epoch 8: val_accuracy did not improve from 0.78284\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1239s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 1.3143 - val_accuracy: 0.7791 - val_loss: 2.3028\n",
      "Epoch 9/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9979 - loss: 1.2449\n",
      "Epoch 9: val_accuracy did not improve from 0.78284\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1251s\u001b[0m 1s/step - accuracy: 0.9979 - loss: 1.2449 - val_accuracy: 0.7756 - val_loss: 2.2655\n",
      "Epoch 10/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9985 - loss: 1.1786\n",
      "Epoch 10: val_accuracy did not improve from 0.78284\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1241s\u001b[0m 1s/step - accuracy: 0.9985 - loss: 1.1786 - val_accuracy: 0.7822 - val_loss: 2.1881\n",
      "Epoch 11/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9986 - loss: 1.1160\n",
      "Epoch 11: val_accuracy improved from 0.78284 to 0.78720, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1284s\u001b[0m 1s/step - accuracy: 0.9986 - loss: 1.1160 - val_accuracy: 0.7872 - val_loss: 2.0895\n",
      "Epoch 12/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9992 - loss: 1.0569\n",
      "Epoch 12: val_accuracy improved from 0.78720 to 0.79216, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1242s\u001b[0m 1s/step - accuracy: 0.9992 - loss: 1.0569 - val_accuracy: 0.7922 - val_loss: 1.9922\n",
      "Epoch 13/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9989 - loss: 1.0018\n",
      "Epoch 13: val_accuracy improved from 0.79216 to 0.79261, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1241s\u001b[0m 1s/step - accuracy: 0.9989 - loss: 1.0018 - val_accuracy: 0.7926 - val_loss: 1.9525\n",
      "Epoch 14/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9989 - loss: 0.9490\n",
      "Epoch 14: val_accuracy did not improve from 0.79261\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1239s\u001b[0m 1s/step - accuracy: 0.9989 - loss: 0.9490 - val_accuracy: 0.7890 - val_loss: 1.9238\n",
      "Epoch 15/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9992 - loss: 0.8990\n",
      "Epoch 15: val_accuracy improved from 0.79261 to 0.79351, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1286s\u001b[0m 1s/step - accuracy: 0.9992 - loss: 0.8990 - val_accuracy: 0.7935 - val_loss: 1.8564\n",
      "Epoch 16/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9991 - loss: 0.8520\n",
      "Epoch 16: val_accuracy did not improve from 0.79351\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1239s\u001b[0m 1s/step - accuracy: 0.9991 - loss: 0.8520 - val_accuracy: 0.7904 - val_loss: 1.8375\n",
      "Epoch 17/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9993 - loss: 0.8073\n",
      "Epoch 17: val_accuracy did not improve from 0.79351\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1281s\u001b[0m 1s/step - accuracy: 0.9993 - loss: 0.8073 - val_accuracy: 0.7899 - val_loss: 1.8229\n",
      "Epoch 18/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9993 - loss: 0.7646\n",
      "Epoch 18: val_accuracy did not improve from 0.79351\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1283s\u001b[0m 1s/step - accuracy: 0.9993 - loss: 0.7646 - val_accuracy: 0.7919 - val_loss: 1.7699\n",
      "Epoch 19/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9994 - loss: 0.7249\n",
      "Epoch 19: val_accuracy improved from 0.79351 to 0.79531, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1285s\u001b[0m 1s/step - accuracy: 0.9994 - loss: 0.7249 - val_accuracy: 0.7953 - val_loss: 1.7341\n",
      "Epoch 20/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9992 - loss: 0.6870\n",
      "Epoch 20: val_accuracy improved from 0.79531 to 0.80042, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1243s\u001b[0m 1s/step - accuracy: 0.9992 - loss: 0.6870 - val_accuracy: 0.8004 - val_loss: 1.6360\n",
      "Epoch 21/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9995 - loss: 0.6509\n",
      "Epoch 21: val_accuracy improved from 0.80042 to 0.80252, saving model to model_best.keras\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1248s\u001b[0m 1s/step - accuracy: 0.9995 - loss: 0.6509 - val_accuracy: 0.8025 - val_loss: 1.5902\n",
      "Epoch 22/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9995 - loss: 0.6166\n",
      "Epoch 22: val_accuracy did not improve from 0.80252\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1273s\u001b[0m 1s/step - accuracy: 0.9995 - loss: 0.6166 - val_accuracy: 0.7976 - val_loss: 1.5743\n",
      "Epoch 23/50\n",
      "\u001b[1m906/906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9992 - loss: 0.5851"
     ]
    }
   ],
   "source": [
    "# Here, we compile the model, initialize a Neptune run to track training, define callbacks (checkpointing, early stopping, etc.)\n",
    "# train the model with model.fit(...), logs metrics to NeptuneHandles interruptions or crashes gracefully and finalizes the run with post-training logs\n",
    "\n",
    "learning_rate = 0.001 # controls how fast the model learns. 0.001 is a good start.\n",
    "epochs = 50 # it will try to train for 50 full passes over the dataset (unless stopped earlier).\n",
    "\n",
    "# Starts a new Neptune run to track the training session.\n",
    "if using_neptune:\n",
    "    run = neptune.init_run(project=\"LSE-Sign-Language2/LSE-Sign-Language\", api_token=neptune_api_token, capture_hardware_metrics=True, capture_stdout=True, capture_stderr=False)\n",
    "    params = {\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"optimizer\": \"SGD\",\n",
    "        \"base_model\": base_model.name,\n",
    "        \"image_side\": image_side,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": bs\n",
    "        }\n",
    "    \n",
    "    run[\"sys/name\"] = f\"{base_model.name}_freeze_{num_layers_to_freeze}_on_{os.path.basename(DATASET_PATH)}\"\n",
    "    run[\"parameters\"] = params\n",
    "    run[\"status\"] = \"running\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# categorical_crossentropy: used for multi-class classification (with one-hot labels) and SGD (Stochastic Gradient Descent) a basic optimizer.\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer= SGD(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "# This save the best model\n",
    "checkpointer = ModelCheckpoint(filepath='model_best.keras', verbose=1, save_best_only=True, monitor = 'val_accuracy', mode = 'max')\n",
    "\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.000001) # this reduce learning rate when val_loss is not improving\n",
    "\n",
    "# Stops training early if the validation loss doesn't improve for 5 epochs\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5) \n",
    "\n",
    "# This function logs training and validation metrics to Neptune after every epoch\n",
    "def epochCallback(epoch, logs):\n",
    "    if using_neptune:\n",
    "        run[\"train/loss\"].log(logs[\"loss\"])\n",
    "        run[\"train/accuracy\"].log(logs[\"accuracy\"])\n",
    "        run[\"eval/loss\"].log(logs[\"val_loss\"])\n",
    "        run[\"eval/accuracy\"].log(logs[\"val_accuracy\"])\n",
    "\n",
    "keyinterrupt = False\n",
    "history = None\n",
    "\n",
    "# This is the actual training loop, uses the normalized, augmented datasets. Uses callbacks to: log to Neptune, save best model, stop early if overfitting\n",
    "try:\n",
    "    history = model.fit(train_ds, validation_data = validation_ds, epochs=epochs,\n",
    "                                callbacks = [\n",
    "    #                                reduce_lr,\n",
    "                                    checkpointer,\n",
    "                                    tf.keras.callbacks.LambdaCallback(on_epoch_end=epochCallback),\n",
    "                                    early_stop\n",
    "                                ])\n",
    "    if using_neptune:\n",
    "        run[\"status\"] = \"finished\"\n",
    "except tf.errors.ResourceExhaustedError:\n",
    "    if using_neptune:\n",
    "        run[\"status\"] = \"crashed-ResourceExhausted\"\n",
    "except KeyboardInterrupt:\n",
    "    keyinterrupt=True\n",
    "    if using_neptune:\n",
    "        run[\"status\"] = \"Interrupted keyboard\"\n",
    "\n",
    "logModelTrainFinished(run, model, validation_ds, class_names)\n",
    "\n",
    "if keyinterrupt:\n",
    "    raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7",
   "language": "python",
   "name": "py3.11.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
